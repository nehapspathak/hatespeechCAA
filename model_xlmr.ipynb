{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnn-V5gJjKMg"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence_transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BbuhYyijKMk"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkvz3KbejKMl"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, XLMRobertaModel\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lDT2ex1jKMn"
      },
      "outputs": [],
      "source": [
        "base_path=\"./data/input/\"\n",
        "model_dir='xlmr'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCWOkvlxjKMn"
      },
      "outputs": [],
      "source": [
        "# # curating dataset\n",
        "# caa_anti = pd.read_excel(f'{base_path}caa_anti.xlsx', sheet_name=\"NK_CAA_Anti\")\n",
        "# caa_pro = pd.read_excel(f'{base_path}caa_pro.xlsx', sheet_name=\"NK\")\n",
        "# fp_anti = pd.read_excel(f'{base_path}fp_anti.xlsx', sheet_name=\"fp_anti\")\n",
        "# fp_pro = pd.read_excel(f'{base_path}fp_pro.xlsx', sheet_name=\"nk\")\n",
        "# # df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdmIDHBmjKMo"
      },
      "outputs": [],
      "source": [
        "# caa_anti['dataset'] = 0\n",
        "# caa_pro['dataset'] = 0\n",
        "# fp_anti['dataset'] = 1\n",
        "# fp_pro['dataset'] = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q0c0YgcjKMp"
      },
      "outputs": [],
      "source": [
        "# df = pd.concat([caa_pro, caa_anti, fp_pro, fp_anti])\n",
        "# df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6ZwI1K1jKMu"
      },
      "outputs": [],
      "source": [
        "# train, test, train_y, test_y = train_test_split(df, df.Stance, test_size=0.3, random_state=42)\n",
        "# train, val, train_y, val_y = train_test_split(train, train_y, random_state=42, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2dqfwXjKMv"
      },
      "outputs": [],
      "source": [
        "# train.to_csv(f'{base_path}train.csv', index = False)\n",
        "# val.to_csv(f'{base_path}val.csv', index=False)\n",
        "# test.to_csv(f'{base_path}test.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMpYxAsnjKMw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "def clean_text(row):\n",
        "    text = []\n",
        "    [text.extend(i.strip().split('ред')) for i in row]\n",
        "    text = [i.strip() for i in text]\n",
        "    text = list(filter(None, text))\n",
        "    return text\n",
        "\n",
        "def clean_dataset():\n",
        "    train = pd.read_csv(f'{base_path}train.csv')\n",
        "    val = pd.read_csv(f'{base_path}val.csv')\n",
        "    test = pd.read_csv(f'{base_path}test.csv')\n",
        "\n",
        "    train.text = train.text.map(lambda x : clean_text(x))\n",
        "    val.text = val.text.map(lambda x : clean_text(x))\n",
        "    test.text = test.text.map(lambda x : clean_text(x))\n",
        "    return train, val, test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdtckE7WjKMx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.stance = self.df.Stance.map({'Anti': 0, 'Pro': 1})\n",
        "        self.sentence_model = None\n",
        "        if model_dir == 'mbert':\n",
        "            self.sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
        "        else:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "            self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        if self.sentence_model:\n",
        "            embeddings = self.sentence_model.encode(\n",
        "                self.df.text.iloc[idx]\n",
        "            )\n",
        "            sample['embeddings'] = torch.from_numpy(embeddings)\n",
        "\n",
        "        else:\n",
        "            embeddings = self.model(**self.tokenizer(self.df.text.iloc[idx], return_tensors='pt', truncation=True, padding=\"max_length\"))\n",
        "            sample['embeddings'] = torch.mean(embeddings.last_hidden_state, axis=1)\n",
        "\n",
        "        sample['stance'] = torch.Tensor([self.stance.iloc[idx]])\n",
        "        sample['dataset'] = torch.Tensor([self.df.dataset.iloc[idx]])\n",
        "        sample['hate'] = torch.Tensor([self.df.Hate.iloc[idx]])\n",
        "\n",
        "        return sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpsGY7FCjKMx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_collate(batch):\n",
        "\n",
        "    dataset, stance, embs, hate = [], [], [], []\n",
        "    for item in batch:\n",
        "        dataset.append(item['dataset'])\n",
        "        stance.append(item['stance'])\n",
        "        hate.append(item['hate'])\n",
        "        embs.append(item['embeddings'])\n",
        "        # print(item['embeddings'].shape)\n",
        "    dataset = pad_sequence(dataset, batch_first=True)\n",
        "    embs = pad_sequence(embs, batch_first=True)\n",
        "    # print(embs.shape)\n",
        "    stance = pad_sequence(stance, batch_first=True)\n",
        "    hate = pad_sequence(hate, batch_first=True)\n",
        "    return embs, dataset.long(), stance.long(), hate.long()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Hl965qBjKMx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        nhead=1,\n",
        "        nlayers=1,\n",
        "        # use_cls=True,\n",
        "        d_model=768\n",
        "    ):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                nhead=nhead,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            nlayers,\n",
        "            norm=None\n",
        "        )\n",
        "        self.dataset_classifier = nn.Linear(d_model, 2)\n",
        "        self.stance_classifier = nn.Linear(d_model, 2)\n",
        "        self.hate_classifier = nn.Linear(d_model, 2)\n",
        "\n",
        "        ## Use [cls] token or pooling output for bail prediction\n",
        "        # self.use_cls = use_cls\n",
        "        self.d_model = d_model\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## x: (batch_size, padded_length, 768)\n",
        "        batch_size = x.size()[0]\n",
        "\n",
        "        x = self.encoder_layer(x)\n",
        "        x = torch.sum(x, dim=1)\n",
        "\n",
        "        stance_logits = self.stance_classifier(x)  ## (batch_size, 2)\n",
        "        dataset_logits = self.dataset_classifier(x) ## (batch_size, 2)\n",
        "        hate_logits = self.hate_classifier(x) ## (batch_size, 2)\n",
        "\n",
        "        return dataset_logits, stance_logits, hate_logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAZVQ1SEjKMy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_func_epoch(epoch, model, dataloader, device, optimizer, scheduler, gradient_accumulation_steps=1):\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    with tqdm(dataloader, unit=\"batch\", total=len(dataloader)) as single_epoch:\n",
        "\n",
        "        for step, batch in enumerate(single_epoch):\n",
        "\n",
        "            single_epoch.set_description(f\"Training- Epoch {epoch}\")\n",
        "\n",
        "            embeddings, dataset_label, stance_label, hate_label = batch\n",
        "\n",
        "            ## Load the inputs to the device\n",
        "            embeddings = embeddings.to(device)\n",
        "            dataset_label = dataset_label.to(device)\n",
        "            stance_label = stance_label.to(device)\n",
        "            hate_label = hate_label.to(device)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass.\n",
        "            dataset_logits, stance_logits, hate_logits = model(embeddings) ## mask\n",
        "\n",
        "            # # Flaten the logits and label-  logits: (b_s, number_of_sentences, 2) - > (b_s * number_of_sentences, 2)\n",
        "            # saliency_logits = saliency_logits.contiguous().view(-1, saliency_logits.size(-1))\n",
        "            # saliency_label = saliency_label.contiguous().view(-1)\n",
        "\n",
        "            ## Calculate the final loss\n",
        "            loss_stance = F.cross_entropy(stance_logits, stance_label.squeeze(1))\n",
        "            loss_dataset = F.cross_entropy(dataset_logits, dataset_label.squeeze(1))\n",
        "            loss_hate = F.cross_entropy(hate_logits, hate_label.squeeze(1))\n",
        "\n",
        "            loss = loss_stance + loss_dataset + loss_hate\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if step % gradient_accumulation_steps == 0 or step == len(dataloader) - 1:\n",
        "                ## Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "                ## torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "                # Update parameters and the learning rate\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "            single_epoch.set_postfix(train_loss=total_loss/(step+1))\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flkcA_1cjKMz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def eval_func_epoch(model, dataloader, device, epoch):\n",
        "\n",
        "    # Put the model into the training mode\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    targets_stance = []\n",
        "    targets_dataset = []\n",
        "    targets_hate = []\n",
        "\n",
        "    predictions_dataset = []\n",
        "    predictions_stance = []\n",
        "    predictions_hate = []\n",
        "\n",
        "    with tqdm(dataloader, unit=\"batch\", total=len(dataloader)) as single_epoch:\n",
        "\n",
        "        for step, batch in enumerate(single_epoch):\n",
        "\n",
        "            single_epoch.set_description(f\"Evaluating- Epoch {epoch}\")\n",
        "\n",
        "            embeddings, dataset_label, stance_label, hate_label = batch\n",
        "\n",
        "            ## Load the inputs to the device\n",
        "            embeddings = embeddings.to(device)\n",
        "            dataset_label = dataset_label.to(device)\n",
        "            stance_label = stance_label.to(device)\n",
        "            hate_label = hate_label.to(device)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass.\n",
        "            with torch.no_grad():\n",
        "                dataset_logits, stance_logits, hate_logits = model(embeddings) ## mask\n",
        "\n",
        "            ## Flaten the logits and label-  logits: (b_s, number_of_sentences, 2) - > (b_s * number_of_sentences, 2)\n",
        "            # saliency_logits = saliency_logits.contiguous().view(-1, saliency_logits.size(-1))\n",
        "            # stance_label = stance_label.contiguous().view(-1)\n",
        "\n",
        "            ## Calculate the final loss\n",
        "            loss_dataset = F.cross_entropy(dataset_logits, dataset_label.squeeze(1))\n",
        "            loss_stance = F.cross_entropy(stance_logits, stance_label.squeeze(1))\n",
        "            loss_hate = F.cross_entropy(hate_logits, hate_label.squeeze(1))\n",
        "\n",
        "            loss = loss_stance + loss_dataset + loss_hate\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            single_epoch.set_postfix(train_loss=total_loss/(step+1))\n",
        "\n",
        "            # Finding predictions\n",
        "            pred_dataset = torch.argmax(dataset_logits, dim=1).flatten().cpu().numpy()\n",
        "            pred_stance = torch.argmax(stance_logits, dim=1).flatten().cpu().numpy()\n",
        "            pred_hate = torch.argmax(hate_logits, dim=1).flatten().cpu().numpy()\n",
        "\n",
        "            predictions_dataset.append(pred_dataset)\n",
        "            predictions_stance.append(pred_stance)\n",
        "            predictions_hate.append(pred_hate)\n",
        "\n",
        "            targets_dataset.append(dataset_label.squeeze(1).cpu().numpy())\n",
        "            targets_stance.append(stance_label.squeeze(1).cpu().numpy())\n",
        "            targets_hate.append(hate_label.squeeze(1).cpu().numpy())\n",
        "\n",
        "            #if step == 10:\n",
        "               #break\n",
        "\n",
        "    targets_dataset = np.concatenate(targets_dataset, axis=0)\n",
        "    targets_stance = np.concatenate(targets_stance, axis=0)\n",
        "    targets_hate = np.concatenate(targets_hate, axis=0)\n",
        "\n",
        "    predictions_dataset = np.concatenate(predictions_dataset, axis=0)\n",
        "    predictions_stance = np.concatenate(predictions_stance, axis=0)\n",
        "    predictions_hate = np.concatenate(predictions_hate, axis=0)\n",
        "\n",
        "    epoch_validation_loss = total_loss/len(dataloader)\n",
        "\n",
        "    report_dataset = classification_report(targets_dataset, predictions_dataset, output_dict=True, labels=[0,1])\n",
        "    report_stance = classification_report(targets_stance, predictions_stance, output_dict=True, labels=[0,1])\n",
        "    report_hate = classification_report(targets_hate, predictions_hate, output_dict=True, labels=[0,1])\n",
        "\n",
        "    cm_d = confusion_matrix(targets_dataset, predictions_dataset, labels=[0,1])\n",
        "    cm_s = confusion_matrix(targets_stance, predictions_stance, labels=[0,1])\n",
        "    cm_h = confusion_matrix(targets_hate, predictions_hate, labels=[0,1])\n",
        "\n",
        "\n",
        "    tn_d, fp_d, fn_d, tp_d = cm_d.ravel()\n",
        "    tn_s, fp_s, fn_s, tp_s = cm_s.ravel()\n",
        "    tn_h, fp_h, fn_h, tp_h = cm_h.ravel()\n",
        "\n",
        "    if epoch == \"TESTING\":\n",
        "        disp = ConfusionMatrixDisplay(cm_d, display_labels=[0,1])\n",
        "        disp.plot()\n",
        "        plt.savefig(f\"./data/output/{model_dir}/dataset.png\", dpi=300)\n",
        "        disp = ConfusionMatrixDisplay(cm_s, display_labels=[0,1])\n",
        "        disp.plot()\n",
        "        plt.savefig(f\"./data/output/{model_dir}/stance.png\", dpi=300)\n",
        "        disp = ConfusionMatrixDisplay(cm_h, display_labels=[0,1])\n",
        "        disp.plot()\n",
        "        plt.savefig(f\"./data/output/{model_dir}/hate.png\", dpi=300)\n",
        "\n",
        "    return epoch_validation_loss, report_dataset, tn_d, fp_d, fn_d, tp_d, report_stance, tn_s, fp_s, fn_s, tp_s, report_hate, tn_h, fp_h, fn_h, tp_h\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejCGI_dQjKMz"
      },
      "outputs": [],
      "source": [
        "pad_sequence([torch.zeros(1,3,4), torch.ones(2,3,4)], batch_first=True).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0UMth2UjKM0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "batch_size = 8\n",
        "gradient_accumulation_steps = 1\n",
        "epochs = 15\n",
        "num_warmup_steps = 0\n",
        "save_model = True\n",
        "model_path = f\"./data/output/{model_dir}/model_epochs_{epochs}.pt\" ## Change after every experiment\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(\"Reading CSV files.\")\n",
        "train, val, test = clean_dataset()\n",
        "\n",
        "train_dataset = Dataset(train)\n",
        "val_dataset = Dataset(val)\n",
        "print(\"Trainng and Validation datasets ready.\")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "cls_bail_embed = torch.ones(1, 1, 768).to(device) ## embedding size = 768\n",
        "d_model=768 ## embedding size = 768\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGc2f5BtjKM1"
      },
      "outputs": [],
      "source": [
        "model = MultiTaskModel(d_model=768)\n",
        "\n",
        "model.to(device)\n",
        "print(\"Model ready.\")\n",
        "## Load weights\n",
        "# loaded_state_dict = torch.load(model_path,  map_location=device)\n",
        "# model.load_state_dict(loaded_state_dict)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5*1e-5)\n",
        "\n",
        "num_train_steps = math.ceil(len(train_dataloader)/gradient_accumulation_steps)  * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps\n",
        ")\n",
        "\n",
        "best_loss = np.inf\n",
        "best_epoch = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV5bW9iHjKM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n---------------------- Epoch: {epoch+1} ---------------------------------- \\n\")\n",
        "    ## Training Loop\n",
        "    train_loss = train_func_epoch(epoch+1, model, train_dataloader, device, optimizer, scheduler)\n",
        "\n",
        "    ## Validation loop\n",
        "    (\n",
        "        val_loss, report_d, tn_d, fp_d, fn_d, tp_d,\n",
        "        report_s, tn_s, fp_s, fn_s, tp_s,\n",
        "        report_h, tn_h, fp_h, fn_h, tp_h\n",
        "    ) = eval_func_epoch(model, val_dataloader, device, epoch+1)\n",
        "\n",
        "    print(f\"\\nEpoch: {epoch+1} | Training loss: {train_loss} | Validation Loss: {val_loss}\")\n",
        "    print()\n",
        "    print(report_d)\n",
        "    print()\n",
        "    print(f\"Dataset ==> TP: {tp_d} | FP: {fp_d} | TN: {tn_d}, FN: {fn_d} \")\n",
        "    print()\n",
        "    print(report_s)\n",
        "    print()\n",
        "    print(f\"Stance ==> TP: {tp_s} | FP: {fp_s} | TN: {tn_s}, FN: {fn_s} \")\n",
        "    print()\n",
        "    print(report_h)\n",
        "    print()\n",
        "    print(f\"Hate ==> TP: {tp_h} | FP: {fp_h} | TN: {tn_h}, FN: {fn_h} \")\n",
        "    print(f\"\\n----------------------------------------------------------------------------\")\n",
        "\n",
        "    ## Save the model\n",
        "    if (val_loss < best_loss) and (save_model == True):\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        best_loss = val_loss\n",
        "        best_epoch = epoch+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR6ExALjjKM1"
      },
      "outputs": [],
      "source": [
        "# best_epoch = 8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9jgWacjKM1"
      },
      "outputs": [],
      "source": [
        "# model = MultiTaskModel(d_model=768)\n",
        "\n",
        "# model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA2C5XhUjKM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # In[5]:\n",
        "\n",
        "# # def eval_func_epoch(model, dataloader, device, epoch):\n",
        "\n",
        "# epoch=\"TESTING\"\n",
        "# dataloader = test_dataloader\n",
        "# # Put the model into the training mode\n",
        "# model.eval()\n",
        "\n",
        "# total_loss = 0\n",
        "\n",
        "# targets_stance = []\n",
        "# targets_dataset = []\n",
        "# targets_hate = []\n",
        "\n",
        "# predictions_dataset = []\n",
        "# predictions_stance = []\n",
        "# predictions_hate = []\n",
        "\n",
        "# with tqdm(dataloader, unit=\"batch\", total=len(dataloader)) as single_epoch:\n",
        "\n",
        "#     for step, batch in enumerate(single_epoch):\n",
        "\n",
        "#         single_epoch.set_description(f\"Evaluating- Epoch {epoch}\")\n",
        "\n",
        "#         embeddings, dataset_label, stance_label, hate_label = batch\n",
        "\n",
        "#         ## Load the inputs to the device\n",
        "#         embeddings = embeddings.to(device)\n",
        "#         dataset_label = dataset_label.to(device)\n",
        "#         stance_label = stance_label.to(device)\n",
        "#         hate_label = hate_label.to(device)\n",
        "\n",
        "#         # Zero out any previously calculated gradients\n",
        "#         model.zero_grad()\n",
        "\n",
        "#         # Perform a forward pass.\n",
        "#         with torch.no_grad():\n",
        "#             dataset_logits, stance_logits, hate_logits = model(embeddings) ## mask\n",
        "\n",
        "#         ## Flaten the logits and label-  logits: (b_s, number_of_sentences, 2) - > (b_s * number_of_sentences, 2)\n",
        "#         # saliency_logits = saliency_logits.contiguous().view(-1, saliency_logits.size(-1))\n",
        "#         # stance_label = stance_label.contiguous().view(-1)\n",
        "\n",
        "#         ## Calculate the final loss\n",
        "#         loss_dataset = F.cross_entropy(dataset_logits, dataset_label.squeeze(1))\n",
        "#         loss_stance = F.cross_entropy(stance_logits, stance_label.squeeze(1))\n",
        "#         loss_hate = F.cross_entropy(hate_logits, hate_label.squeeze(1))\n",
        "\n",
        "#         loss = loss_stance + loss_dataset + loss_hate\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         single_epoch.set_postfix(train_loss=total_loss/(step+1))\n",
        "\n",
        "#         # Finding predictions\n",
        "#         pred_dataset = torch.argmax(dataset_logits, dim=1).flatten().cpu().numpy()\n",
        "#         pred_stance = torch.argmax(stance_logits, dim=1).flatten().cpu().numpy()\n",
        "#         pred_hate = torch.argmax(hate_logits, dim=1).flatten().cpu().numpy()\n",
        "\n",
        "#         predictions_dataset.append(pred_dataset)\n",
        "#         predictions_stance.append(pred_stance)\n",
        "#         predictions_hate.append(pred_hate)\n",
        "\n",
        "#         targets_dataset.append(dataset_label.squeeze(1).cpu().numpy())\n",
        "#         targets_stance.append(stance_label.squeeze(1).cpu().numpy())\n",
        "#         targets_hate.append(hate_label.squeeze(1).cpu().numpy())\n",
        "\n",
        "#         #if step == 10:\n",
        "#            #break\n",
        "\n",
        "# targets_dataset = np.concatenate(targets_dataset, axis=0)\n",
        "# targets_stance = np.concatenate(targets_stance, axis=0)\n",
        "# targets_hate = np.concatenate(targets_hate, axis=0)\n",
        "\n",
        "# predictions_dataset = np.concatenate(predictions_dataset, axis=0)\n",
        "# predictions_stance = np.concatenate(predictions_stance, axis=0)\n",
        "# predictions_hate = np.concatenate(predictions_hate, axis=0)\n",
        "\n",
        "# epoch_validation_loss = total_loss/len(dataloader)\n",
        "\n",
        "# report_dataset = classification_report(targets_dataset, predictions_dataset, output_dict=True, labels=[0,1])\n",
        "# report_stance = classification_report(targets_stance, predictions_stance, output_dict=True, labels=[0,1])\n",
        "# report_hate = classification_report(targets_hate, predictions_hate, output_dict=True, labels=[0,1])\n",
        "\n",
        "# cm_d = confusion_matrix(targets_dataset, predictions_dataset, labels=[0,1])\n",
        "# cm_s = confusion_matrix(targets_stance, predictions_stance, labels=[0,1])\n",
        "# cm_h = confusion_matrix(targets_hate, predictions_hate, labels=[0,1])\n",
        "\n",
        "\n",
        "# tn_d, fp_d, fn_d, tp_d = cm_d.ravel()\n",
        "# tn_s, fp_s, fn_s, tp_s = cm_s.ravel()\n",
        "# tn_h, fp_h, fn_h, tp_h = cm_h.ravel()\n",
        "\n",
        "# # if epoch == \"TESTING\":\n",
        "# #     disp = ConfusionMatrixDisplay(cm_d, display_labels=[0,1])\n",
        "# #     disp.figure_.savefig(\"./dataset.png\", dpi=300)\n",
        "# #     disp = ConfusionMatrixDisplay(cm_s, display_labels=[0,1])\n",
        "# #     disp.figure_.savefig(\"./stance.png\", dpi=300)\n",
        "# #     disp = ConfusionMatrixDisplay(cm_h, display_labels=[0,1])\n",
        "# #     disp.figure_.savefig(\"./hate.png\", dpi=300)\n",
        "\n",
        "# return epoch_validation_loss, report_dataset, tn_d, fp_d, fn_d, tp_d, report_stance, tn_s, fp_s, fn_s, tp_s, report_hate, tn_h, fp_h, fn_h, tp_h\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0MzPC0sjKM2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# disp = ConfusionMatrixDisplay(cm_d, display_labels=[0,1])\n",
        "# disp.plot()\n",
        "# plt.savefig(\"./dataset.png\", dpi=300)\n",
        "# disp = ConfusionMatrixDisplay(cm_s, display_labels=[0,1])\n",
        "# disp.plto()\n",
        "# plt.savefig(\"./stance.png\", dpi=300)\n",
        "# disp = ConfusionMatrixDisplay(cm_h, display_labels=[0,1])\n",
        "# disp.plot()\n",
        "# plt.savefig(\"./hate.png\", dpi=300)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm4ZNMGhjKM2"
      },
      "outputs": [],
      "source": [
        "## After training get the score on test set\n",
        "test_dataset = Dataset(test)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate\n",
        ")\n",
        "\n",
        "## Load weights\n",
        "loaded_state_dict = torch.load(model_path,  map_location=device)\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "\n",
        "print(f\"\\n---------------------- Testing best model (at epoch: {best_epoch} )---------------------------------- \\n\")\n",
        "# test_loss,report, tn, fp, fn, tp = eval_func_epoch(model, test_dataloader, device, \"TESTING\")\n",
        "(\n",
        "    test_loss, report_d, tn_d, fp_d, fn_d, tp_d,\n",
        "    report_s, tn_s, fp_s, fn_s, tp_s,\n",
        "    report_h, tn_h, fp_h, fn_h, tp_h\n",
        ") = eval_func_epoch(model, test_dataloader, device, \"TESTING\")\n",
        "\n",
        "print(f\"\\nTest loss: {test_loss}\")\n",
        "print()\n",
        "print(report_d)\n",
        "print()\n",
        "print(f\"Dataset ==> TP: {tp_d} | FP: {fp_d} | TN: {tn_d}, FN: {fn_d} \")\n",
        "print()\n",
        "print(report_s)\n",
        "print()\n",
        "print(f\"Stance ==> TP: {tp_s} | FP: {fp_s} | TN: {tn_s}, FN: {fn_s} \")\n",
        "print()\n",
        "print(report_h)\n",
        "print()\n",
        "print(f\"Hate ==> TP: {tp_h} | FP: {fp_h} | TN: {tn_h}, FN: {fn_h} \")\n",
        "print(f\"\\n----------------------------------------------------------------------------\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3LN_Ad1jKM2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuaWCtZMjKM2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(f\"./data/output/{model_dir}/dataset.json\",\"w\") as f:\n",
        "    json.dump(report_d ,f,indent=4)\n",
        "\n",
        "with open(f\"./data/output/{model_dir}/stance.json\",\"w\") as f:\n",
        "    json.dump(report_s ,f,indent=4)\n",
        "\n",
        "with open(f\"./data/output/{model_dir}/hate.json\",\"w\") as f:\n",
        "    json.dump(report_h ,f,indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VODXcJGDjKM2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "944f20c8da70e9450d1fa0eb874e26ae64087dff02fdb8273555f677957db2f2"
    },
    "kernelspec": {
      "display_name": "Python 3.6.5 ('script': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}